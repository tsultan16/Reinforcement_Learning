{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is an implementation of Multi Agent Monte Carlo Tree Search (MCTS) for the game of tic tac toe. The game is turn based and the search tree levels alternate between player and opponent nodes, i.e. all sucessors of a player node are opponent nodes and vice versa. The goal of the algorithm is to iteratively build up a search tree and at the end of the iterations, choose the best possible actions based on the values of the successor nodes of the root node. There are 4 main steps in the algorithm, always starting from the root node:\n",
    "\n",
    "1) `Selection`: This step involves traversing down the tree until a leaf node is found (a leaf node is a node which has not been expanded before, i.e. it has no children). The traversal is done using an exploration-exploitation strategy (UCT) which balances nodes with higher value with random exploration. (The value of a node is the accumulated reward for the player at that node in proportion to the number of times that node has been visited/traversed.)\n",
    "\n",
    "2) `Expansion`: The selcted leaf node is expanded (i.e. all of its children are generated), and the one of its children is picked at random and it's action is executed.\n",
    "\n",
    "3) `Simulation`: Then a simulation is run from the game state resulting from executing the action of the random child. \n",
    "\n",
    "4) `Backpropagation`: Rewards from the simulation results are accumulated on every node along the path from the random child to the root. In this case the reward from the simuation is 1 for the winning player and 0 for the loser or 0 for both if it's a draw.  \n",
    "\n",
    "After many iterations, we can then extract the best action by choosing the action of the child node of the root which has the best value, i.e. best accuumulated reward to number of visitc ratio. This type of strategy is called a \"self-play\" tree policy because the player and opponent both share the same state-action space, so the opponentn can be thought of as the player playing against itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self) -> None:\n",
    "        self.board = [' '] * 9 # initially empty board\n",
    "        self.current_player = 'X' # player X gets first turn\n",
    "\n",
    "    # returns list of empty positions on the board\n",
    "    def get_legal_moves(self):    \n",
    "        return [i for i, cell in enumerate(self.board) if cell == ' ']\n",
    "    \n",
    "\n",
    "    # player move, marks a position on the board, then switches turn\n",
    "    def make_move(self, move):\n",
    "        self.board[move] = self.current_player\n",
    "        # switch turn\n",
    "        self.current_player = 'O' if self.current_player == 'X' else 'X'\n",
    "\n",
    "    # checks if game is over\n",
    "    def is_terminal(self):\n",
    "        winning_combinations = [(0,1,2), (3,4,5), (6,7,8),  # rows\n",
    "                                (0,3,6), (1,4,7), (2,5,8),  # cols\n",
    "                                (0,4,8), (2,4,6)            # diagonals \n",
    "                                ]                  \n",
    "        \n",
    "        for combo in winning_combinations:\n",
    "            if self.board[combo[0]] == self.board[combo[1]] == self.board[combo[2]] != ' ':\n",
    "                return True \n",
    "\n",
    "    # returns winner is there is one\n",
    "    def gets_winner(self):\n",
    "        winning_combinations = [(0,1,2), (3,4,5), (6,7,8),  # rows\n",
    "                                (0,3,6), (1,4,7), (2,5,8),  # cols\n",
    "                                (0,4,8), (2,4,6)            # diagonals \n",
    "                                ]                  \n",
    "        \n",
    "        for combo in winning_combinations:\n",
    "            if self.board[combo[0]] == self.board[combo[1]] == self.board[combo[2]] != ' ':\n",
    "                return self.board[combo[0]] \n",
    "            \n",
    "        return ' '    \n",
    " \n",
    "    # display the game board\n",
    "    def print_board(self):\n",
    "        print('---------')\n",
    "        for i in range(0, 9, 3):\n",
    "            print(self.board[i], '|', self.board[i+1], '|', self.board[i+2])\n",
    "        print('---------')\n",
    "\n",
    "\n",
    "\n",
    "class Node:\n",
    "\n",
    "    next_node_id = 0\n",
    "\n",
    "    def __init__(self, move=None, parent=None) -> None:\n",
    "        self.move = move\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.wins = 0\n",
    "        self.visits = 0\n",
    "        self.id = Node.next_node_id\n",
    "        Node.next_node_id += 1\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)    \n",
    "\n",
    "    def update(self, win):\n",
    "        # update visit stats\n",
    "        self.visits += 1\n",
    "        # accumulate rewards\n",
    "        self.wins += win\n",
    "\n",
    "\n",
    "\n",
    "class MultiAgentMCTS:\n",
    "    def __init__(self, exploration_constant=1.4, iterations=10) -> None:\n",
    "        self.exploration_constant = exploration_constant\n",
    "        self.iterations = iterations\n",
    "\n",
    "\n",
    "    # UCB selection of successor node\n",
    "    def select_child(self, node):\n",
    "        total_visits = node.visits    # sum(child.visits for child in node.children)\n",
    "        log_total_visits = math.log(total_visits)\n",
    "\n",
    "        best_score = float(\"-inf\")\n",
    "        best_child = None\n",
    "\n",
    "        # find child node with highest score\n",
    "        for child in node.children:\n",
    "            exploit_term = child.wins/child.visits\n",
    "            explore_term = self.exploration_constant * math.sqrt(2.0*log_total_visits/child.visits) \n",
    "            score = exploit_term + explore_term\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_child = child\n",
    "\n",
    "        return best_child    \n",
    "\n",
    "    # traverses down the tree and selects an unexplored/unexpanded child node which is not a termninal state\n",
    "    def select(self, node, state):\n",
    "        while node.children and not state.is_terminal():\n",
    "            # select best child according to UCB bandit\n",
    "            child = self.select_child(node)\n",
    "            # execute it's move\n",
    "            state.make_move(child.move)\n",
    "            node = child\n",
    "        \n",
    "        return node, state    \n",
    "    \n",
    "\n",
    "    '''\n",
    "    def expand_partial(self, selected_node, state):\n",
    "        legal_moves = state.get_legal_moves()\n",
    "        unexplored_moves = [move for move in legal_moves if not any(child.move == move for child in selected_node.children)]\n",
    "        if unexplored_moves:\n",
    "            # randomly pick one of the unexplored actions available to selected node and generate a child/successor node from it\n",
    "            move = random.choice(unexplored_moves) \n",
    "            state.make_move(move)\n",
    "            new_node = Node(move, node)\n",
    "            node = node.add_child(new_node)\n",
    "\n",
    "        return new_node, state\n",
    "    '''\n",
    "\n",
    "\n",
    "    # generates all children of a node given the game state represented by that node\n",
    "    def expand(self, node, game_state):\n",
    "        # get all available actions for this node\n",
    "        legal_moves = game_state.get_legal_moves()\n",
    "\n",
    "        if len(legal_moves) == 0:\n",
    "            game_state.print_board()\n",
    "            raise RuntimeError(\"Error! No legal moves found from this state!\")\n",
    "\n",
    "        # generate all successors\n",
    "        for move in legal_moves:\n",
    "            new_child = Node(move=move, parent=node)\n",
    "            node.add_child(new_child)    \n",
    "\n",
    "        return node\n",
    "\n",
    "\n",
    "    # random/monte carlo simulation to terminal state\n",
    "    def simulate(self, state):\n",
    "        while not state.is_terminal():\n",
    "            legal_moves = state.get_legal_moves()\n",
    "            move = random.choice(legal_moves)\n",
    "            state.make_move(move)\n",
    "\n",
    "        return state    \n",
    "\n",
    "\n",
    "    # backpropagate the simulation rewards up to root node\n",
    "    def backpropagate(self, node, state):\n",
    "        winner =  state.get_winner()\n",
    "        while node is not None: \n",
    "            win = 1 if winner == node.move else 0\n",
    "            # update node stats\n",
    "            node.update(win)\n",
    "            node = node.parent   \n",
    "\n",
    "\n",
    "    def search(self, game_state):\n",
    "        # create a root node\n",
    "        root = Node()\n",
    "        # expand the root node\n",
    "        root = self.expand(root, game_state)\n",
    "\n",
    "        # run MCTS iterations\n",
    "        for _ in range(self.iterations):\n",
    "            node = root\n",
    "            # make a copy of the initial game state\n",
    "            state = TicTacToe()\n",
    "            state.board = list(game_state.board)\n",
    "\n",
    "            # select leaf node\n",
    "            selected_node, state = self.select(node, state)\n",
    "        \n",
    "            # expand leaf node and pick one of its successors at random and execute it's move\n",
    "            expanded_node, state = self.expand(selected_node, state)\n",
    "            random_child = random.choice(expanded_node.children)\n",
    "            state.make_move(random_child.move)\n",
    "\n",
    "            # simulation \n",
    "            state = self.simulate(state)\n",
    "\n",
    "            # backpropagation\n",
    "            self.backpropagate(random_child, state)\n",
    "\n",
    "        # after iterations are done, return the best move (i.e. the child with highest value)\n",
    "        best_move = max(root.children, key = lambda child: child.wins/child.visits).move \n",
    "        \n",
    "        return best_move           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
