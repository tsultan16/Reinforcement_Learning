{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math, time, copy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is an implementation of Multi Agent Monte Carlo Tree Search (MCTS) for the game of tic tac toe. The game is turn based and the search tree levels alternate between player and opponent nodes, i.e. all sucessors of a player node are opponent nodes and vice versa. The goal of the algorithm is to iteratively build up a search tree and at the end of the iterations, choose the best possible actions based on the values of the successor nodes of the root node. There are 4 main steps in the algorithm, always starting from the root node:\n",
    "\n",
    "1) `Selection`: This step involves traversing down the tree until a leaf node is found (a leaf node is a node which has not been expanded before, i.e. it has no children). The traversal is done using an exploration-exploitation strategy (UCT) which balances nodes with higher value with random exploration. (The value of a node is the accumulated reward for the player at that node in proportion to the number of times that node has been visited/traversed.)\n",
    "\n",
    "2) `Expansion`: The selcted leaf node is expanded (i.e. all of its children are generated), and the one of its children is picked at random and it's action is executed.\n",
    "\n",
    "3) `Simulation`: Then a simulation is run from the game state resulting from executing the action of the random child. \n",
    "\n",
    "4) `Backpropagation`: Rewards from the simulation results are accumulated on every node along the path from the random child to the root. In this case the reward from the simuation is 1 for the winning player and 0 for the loser or 0 for both if it's a draw.  \n",
    "\n",
    "After many iterations, we can then extract the best action by choosing the action of the child node of the root which has the best value, i.e. best accuumulated reward to number of visitc ratio. This type of strategy is called a \"self-play\" tree policy because the player and opponent both share the same state-action space, so the opponentn can be thought of as the player playing against itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, first_move = 'X') -> None:\n",
    "        self.board = [' '] * 9 # initially empty board\n",
    "        self.current_player = first_move # player X gets first turn\n",
    "\n",
    "    # returns list of empty positions on the board\n",
    "    def get_legal_moves(self):    \n",
    "        return [i for i, cell in enumerate(self.board) if cell == ' ']\n",
    "    \n",
    "\n",
    "    # player move, marks a position on the board, then switches turn\n",
    "    def make_move(self, move):\n",
    "        self.board[move] = self.current_player\n",
    "        # switch turn\n",
    "        self.current_player = 'O' if self.current_player == 'X' else 'X'\n",
    "\n",
    "\n",
    "    # checks if game is over\n",
    "    def is_terminal(self):\n",
    "        winning_combinations = [(0,1,2), (3,4,5), (6,7,8),  # rows\n",
    "                                (0,3,6), (1,4,7), (2,5,8),  # cols\n",
    "                                (0,4,8), (2,4,6)            # diagonals \n",
    "                                ]                  \n",
    "        # check for winning combination        \n",
    "        for combo in winning_combinations:\n",
    "            if self.board[combo[0]] == self.board[combo[1]] == self.board[combo[2]] != ' ':\n",
    "                return True \n",
    "\n",
    "        # check if board has no empty position\n",
    "        if len(self.get_legal_moves()) == 0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "    # returns winner is there is one\n",
    "    def get_winner(self):\n",
    "        winning_combinations = [(0,1,2), (3,4,5), (6,7,8),  # rows\n",
    "                                (0,3,6), (1,4,7), (2,5,8),  # cols\n",
    "                                (0,4,8), (2,4,6)            # diagonals \n",
    "                                ]                  \n",
    "        \n",
    "        for combo in winning_combinations:\n",
    "            if self.board[combo[0]] == self.board[combo[1]] == self.board[combo[2]] != ' ':\n",
    "                return self.board[combo[0]] \n",
    "            \n",
    "        return ' '    \n",
    " \n",
    "    # display the game board\n",
    "    def print_board(self):\n",
    "        print('---------')\n",
    "        for i in range(0, 9, 3):\n",
    "            print(self.board[i], '|', self.board[i+1], '|', self.board[i+2])\n",
    "        print('---------')\n",
    "\n",
    "\n",
    "\n",
    "class Node:\n",
    "\n",
    "    next_node_id = 0\n",
    "\n",
    "    def __init__(self, move=None, parent=None) -> None:\n",
    "        self.move = move\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.wins = 0\n",
    "        self.visits = 0\n",
    "        self.id = Node.next_node_id\n",
    "        Node.next_node_id += 1\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)    \n",
    "\n",
    "    def update(self, win):\n",
    "        # update visit stats\n",
    "        self.visits += 1\n",
    "        # accumulate rewards\n",
    "        self.wins += win\n",
    "\n",
    "\n",
    "\n",
    "class MultiAgentMCTS:\n",
    "    def __init__(self, exploration_constant=5.0, iterations=10) -> None:\n",
    "        self.exploration_constant = exploration_constant\n",
    "        self.iterations = iterations\n",
    "\n",
    "\n",
    "    # UCB selection of successor node\n",
    "    def select_child_uct(self, node):\n",
    "\n",
    "        total_visits = node.visits    # sum(child.visits for child in node.children)\n",
    "        best_score = float(\"-inf\")\n",
    "        best_child = None\n",
    "\n",
    "        # find child node with highest score\n",
    "        for child in node.children:\n",
    "            # avoid division by zero\n",
    "            if child.visits == 0 or total_visits == 0:\n",
    "                score = float(\"inf\")\n",
    "            else:    \n",
    "                log_total_visits = math.log(total_visits)\n",
    "                exploit_term = child.wins/child.visits\n",
    "                explore_term = self.exploration_constant * math.sqrt(2.0*log_total_visits/child.visits)         \n",
    "                score = exploit_term + explore_term\n",
    "                \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_child = [child]\n",
    "            elif score == best_score:\n",
    "                 best_child += [child]   \n",
    "\n",
    "            # if there are multiple best children, pick one randomly\n",
    "        return random.choice(best_child)\n",
    "        \n",
    "\n",
    "    # traverses down the tree and selects an unexplored/unexpanded child node which is not a termninal state\n",
    "    def select(self, node, state):\n",
    "        while node.children and not state.is_terminal():\n",
    "            # select best child according to UCB bandit\n",
    "            child = self.select_child_uct(node)\n",
    "            # execute it's move\n",
    "            state.make_move(child.move)\n",
    "            node = child\n",
    "        \n",
    "        return node, state    \n",
    "    \n",
    "\n",
    "    '''\n",
    "    def expand_partial(self, selected_node, state):\n",
    "        legal_moves = state.get_legal_moves()\n",
    "        unexplored_moves = [move for move in legal_moves if not any(child.move == move for child in selected_node.children)]\n",
    "        if unexplored_moves:\n",
    "            # randomly pick one of the unexplored actions available to selected node and generate a child/successor node from it\n",
    "            move = random.choice(unexplored_moves) \n",
    "            state.make_move(move)\n",
    "            new_node = Node(move, node)\n",
    "            node = node.add_child(new_node)\n",
    "\n",
    "        return new_node, state\n",
    "    '''\n",
    "\n",
    "\n",
    "    # generates all children of a node given the game state represented by that node\n",
    "    def expand(self, node, game_state, init=False):\n",
    "        # get all available actions for this node\n",
    "        #if init:\n",
    "        #    print(f\"initial expansion of root node: Initial game state board:\")\n",
    "        #    game_state.print_board()\n",
    "        \n",
    "        legal_moves = game_state.get_legal_moves()\n",
    "\n",
    "        if len(legal_moves) == 0:\n",
    "            #print(f\"ERROR!!!!\")\n",
    "            game_state.print_board()\n",
    "            raise RuntimeError(\"Error! No legal moves found from this state!\")\n",
    "\n",
    "        # generate all successors\n",
    "        for move in legal_moves:\n",
    "            new_child = Node(move=move, parent=node)\n",
    "            node.add_child(new_child)    \n",
    "\n",
    "        return node\n",
    "\n",
    "\n",
    "    # random/monte carlo simulation to terminal state\n",
    "    def simulate(self, state):\n",
    "        while not state.is_terminal():\n",
    "            legal_moves = state.get_legal_moves()\n",
    "            #state.print_board()\n",
    "            #print(f\"Available moves for simulation\")\n",
    "            move = random.choice(legal_moves)\n",
    "            state.make_move(move)\n",
    "\n",
    "        #print(f\"Simulation completed:\")\n",
    "        #state.print_board()\n",
    "        \n",
    "        return state    \n",
    "\n",
    "\n",
    "    # backpropagate the simulation rewards up to root node\n",
    "    def backpropagate(self, node, state):\n",
    "        winner =  state.get_winner()\n",
    "        #print(f\"Winner: {winner if winner != ' ' else 'draw'}\")\n",
    "        while node is not None: \n",
    "            win = 1 if winner == node.move else 0\n",
    "            #print(f\"Backpropagation, node_id = {node.id}, move = {node.move}, win = {win}, parent_id = {node.parent.id if node.parent != None else None}\")\n",
    "            # update node stats\n",
    "            node.update(win)\n",
    "            node = node.parent   \n",
    "    \n",
    "    def get_best_move(self, root, greedy=True):\n",
    "        best_child = []\n",
    "        best_score = float(\"-inf\")\n",
    "        scores = []\n",
    "\n",
    "\n",
    "        for child in root.children:\n",
    "            if child.visits > 0:\n",
    "                score = child.wins/child.visits\n",
    "            else: \n",
    "                score = float(\"-inf\")\n",
    "\n",
    "            scores.append(score)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_child = [child]\n",
    "            elif score == best_score:\n",
    "                best_child.append(child)\n",
    "\n",
    "\n",
    "        # greedy always picks the child node with highest value\n",
    "        if greedy:\n",
    "            # if multiple best child, pick one at random\n",
    "            return random.choice(best_child).move    \n",
    "\n",
    "        # softmax/temperature chooses according to a probability distribution\n",
    "        else:\n",
    "\n",
    "            temperature = 0.5 #0.9\n",
    "            # exponentiate the scores\n",
    "            scores_exp = []\n",
    "            for score in scores:\n",
    "                if score > float(\"-inf\"):\n",
    "                    scores_exp.append(math.exp(score/temperature))\n",
    "                else:\n",
    "                    scores_exp.append(0.0)\n",
    "\n",
    "            sum_exp = sum(scores_exp)\n",
    "            # if all scores are zero, then just pick a child at random\n",
    "            if sum_exp == 0.0:\n",
    "                return random.choice(root.children).move\n",
    "                \n",
    "            probabilities = [score_exp/sum_exp for score_exp in scores_exp]   \n",
    "            # add a tiny bit of random noise to these probabilities\n",
    "            noise_magnitude = 0.00001 * (min(probabilities) + max (probabilities))\n",
    "            probabilities = [(p + noise_magnitude*random.random()) for p in probabilities]\n",
    "\n",
    "            # sample a child node according to thsi probability distribution\n",
    "            child_index = random.choices(list(enumerate(root.children)), weights=probabilities, k=1)[0][0]\n",
    "            #print(f\"probabilities: {probabilities}, child_index = {child_index}\")\n",
    "            child_move = root.children[child_index].move\n",
    "            return child_move\n",
    "\n",
    "\n",
    "\n",
    "    # performs monte carlo tree search iterations (we create a new tree every time, could maybe reuse parts of the same tree)\n",
    "    def search(self, game_state, timeout=1.0):\n",
    "        # create a root node \n",
    "        root = Node(move=game_state.current_player, parent=None)\n",
    "        # expand the root node\n",
    "        root = self.expand(root, game_state, init=True)\n",
    "\n",
    "        # start the timer\n",
    "        start_time = time.time()\n",
    "        current_time = time.time()\n",
    "        num_iterations = 0\n",
    "\n",
    "        #print(f\"Game state before MCTS, player turn: {game_state.current_player}, Board: \")\n",
    "        #game_state.print_board()\n",
    "\n",
    "\n",
    "        # run MCTS iteration until time out\n",
    "        while current_time < start_time + timeout:\n",
    "            node = root\n",
    "            # make a copy of the initial game state\n",
    "            state = TicTacToe()\n",
    "            state.board = copy.deepcopy(game_state.board)\n",
    "            state.current_player = game_state.current_player\n",
    "\n",
    "            #print(f\"Game state board at beginning of iteration:\")\n",
    "            #state.print_board()\n",
    "\n",
    "            # select leaf node\n",
    "            selected_node, state = self.select(node, state)\n",
    "        \n",
    "            #print(f\"Game state board after node selection:\")\n",
    "            #state.print_board()\n",
    "\n",
    "            # carry out steps 2-4 if the selected node is not a terminal state\n",
    "            if not state.is_terminal():\n",
    "                # expand leaf node and pick one of its successors at random and execute it's move\n",
    "                expanded_node = self.expand(selected_node, state)\n",
    "                random_child = random.choice(expanded_node.children)\n",
    "                state.make_move(random_child.move)\n",
    "\n",
    "                # simulation \n",
    "                state = self.simulate(state)\n",
    "\n",
    "                # backpropagation\n",
    "                self.backpropagate(random_child, state)\n",
    "\n",
    "            #print(f\"Iterations# {num_iterations}, Root node_id :{root.id}, num vists = {root.visits}, wins = {root.wins}, value = {root.wins/root.visits}\")\n",
    "            \n",
    "\n",
    "            current_time = time.time()  \n",
    "            num_iterations += 1\n",
    "\n",
    "\n",
    "        # after iterations are done, return the best move (i.e. the child with highest value)\n",
    "        best_move = self.get_best_move(root) \n",
    "        #print(f\"# of Iterations {num_iterations}, Root node_id :{root.id}, Player: {root.move}, num vists = {root.visits}, wins = {root.wins}, value = {(root.wins/root.visits) if root.visits > 0 else None}\")\n",
    "        #print(f\"Num iterations: {num_iterations}, Best move: {best_move}\")\n",
    "        \n",
    "        \n",
    "        #print(f\"Game state after MCTS, player turn: {game_state.current_player}, Board: \")\n",
    "        #game_state.print_board()\n",
    "\n",
    "        return best_move           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# instantiate a game oject\\ngame = TicTacToe()\\n\\n# instantiate mcts solver\\nmcts_solver = MultiAgentMCTS(iterations=300)\\n\\nmcts_solver.search(game)'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# instantiate a game oject\n",
    "game = TicTacToe()\n",
    "\n",
    "# instantiate mcts solver\n",
    "mcts_solver = MultiAgentMCTS(iterations=300)\n",
    "\n",
    "mcts_solver.search(game)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a game of tictac toe played by MCTS agent against itself\n",
    "def self_play(mcts_timeout=1.0, random_X=False, random_O=False):\n",
    "    # instantiate a game oject\n",
    "    game_state = TicTacToe()\n",
    "    # instantiate mcts solver\n",
    "    mcts_solver = MultiAgentMCTS()\n",
    "\n",
    "    #game_state.print_board()\n",
    "\n",
    "    while not game_state.is_terminal():\n",
    "        # alternating player-ooponent move\n",
    "        #print(f\"Player {game_state.current_player} makes a move:\")\n",
    "\n",
    "        if (game_state.current_player == 'X' and random_X) or (game_state.current_player == 'O' and random_O):\n",
    "            # pick random action for opponent\n",
    "            move = random.choice(game_state.get_legal_moves())\n",
    "        else:    \n",
    "            move = mcts_solver.search(game_state, mcts_timeout)\n",
    "            \n",
    "        game_state.make_move(move)\n",
    "        \n",
    "        #game_state.print_board()\n",
    " \n",
    "\n",
    "    winner =  game_state.get_winner()\n",
    "    #print(f\"Winner: {winner if winner != ' ' else 'draw'}\")\n",
    "    \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform many self-plays and compute win rates for each player\n",
    "def win_rate(num_games=500):\n",
    "    playerX = 'X'\n",
    "    playerO = 'O'\n",
    "    draw = ' '\n",
    "    wins = {playerX: 0, playerO: 0, draw: 0}\n",
    "    total_games = 0\n",
    "\n",
    "    with tqdm(total=num_games, ncols=80, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}') as pbar:\n",
    "        for i in range(num_games):\n",
    "            winner = self_play(mcts_timeout=0.01, random_X=False, random_O=True)\n",
    "            wins[winner] =  wins[winner] + 1\n",
    "            total_games += 1\n",
    "            #first_move = 'O' if first_move == 'X' else 'X'\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Games played = {total_games}, Player {playerX} win rate = {wins[playerX]/total_games}, Player {playerO} win rate = {wins[playerO]/total_games}, draw rate = {wins[draw]/total_games}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m win_rate()\n",
      "\u001b[1;32m/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m first_move \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_games):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     winner \u001b[39m=\u001b[39m self_play(mcts_timeout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, random_X\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, random_O\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     wins[winner] \u001b[39m=\u001b[39m  wins[winner] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     total_games \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     move \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(game_state\u001b[39m.\u001b[39mget_legal_moves())\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39melse\u001b[39;00m:    \n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     move \u001b[39m=\u001b[39m mcts_solver\u001b[39m.\u001b[39;49msearch(game_state, mcts_timeout)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m game_state\u001b[39m.\u001b[39mmake_move(move)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#game_state.print_board()\u001b[39;00m\n",
      "\u001b[1;32m/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=267'>268</a>\u001b[0m state\u001b[39m.\u001b[39mcurrent_player \u001b[39m=\u001b[39m game_state\u001b[39m.\u001b[39mcurrent_player\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=269'>270</a>\u001b[0m \u001b[39m#print(f\"Game state board at beginning of iteration:\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=270'>271</a>\u001b[0m \u001b[39m#state.print_board()\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=271'>272</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=272'>273</a>\u001b[0m \u001b[39m# select leaf node\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=273'>274</a>\u001b[0m selected_node, state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselect(node, state)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=275'>276</a>\u001b[0m \u001b[39m#print(f\"Game state board after node selection:\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=276'>277</a>\u001b[0m \u001b[39m#state.print_board()\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=277'>278</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=278'>279</a>\u001b[0m \u001b[39m# carry out steps 2-4 if the selected node is not a terminal state\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=279'>280</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m state\u001b[39m.\u001b[39mis_terminal():\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=280'>281</a>\u001b[0m     \u001b[39m# expand leaf node and pick one of its successors at random and execute it's move\u001b[39;00m\n",
      "\u001b[1;32m/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\u001b[39mself\u001b[39m, node, state):\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m node\u001b[39m.\u001b[39mchildren \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m state\u001b[39m.\u001b[39mis_terminal():\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m         \u001b[39m# select best child according to UCB bandit\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m         child \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselect_child_uct(node)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m         \u001b[39m# execute it's move\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tanzid/Code/UniMelb/Semester2/COMP_90054_AI_Planning/practice_code/reinforcement_learning/simple_mcts_tictactoe.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=121'>122</a>\u001b[0m         state\u001b[39m.\u001b[39mmake_move(child\u001b[39m.\u001b[39mmove)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "win_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### During self play, the player who gets the first move seems to always have a disproportionately higher win rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_bar_meter(total_steps):\n",
    "    with tqdm(total=total_steps, ncols=80, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}') as pbar:\n",
    "        for _ in range(total_steps):\n",
    "            # Do some work here\n",
    "            time.sleep(0.1)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 100/100\n"
     ]
    }
   ],
   "source": [
    "status_bar_meter(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
