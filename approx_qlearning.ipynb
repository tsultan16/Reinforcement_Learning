{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Q learning: Linear Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a linear Q function class\n",
    "class LinearQ(QFunction):\n",
    "\n",
    "    def __init__(self, mdp, features):\n",
    "        self.mdp = mdp\n",
    "        self.features = features\n",
    "        # initialize weights to zero\n",
    "        num_weights = self.features.get_num_actions() * self.features.get_num_features() \n",
    "        self.weights = np.zeros(shape=(num_weights)) \n",
    "\n",
    "        # initialize value table\n",
    "        self.V = {}\n",
    "        for state in (self.mdp.states):\n",
    "            self.V[state] = 0.0\n",
    "\n",
    "    # update the weights\n",
    "    def update(self, state, action, old, delta):\n",
    "        # extract features from state\n",
    "        feature_values = np.array(self.features.extract(state, action))\n",
    "        # update weights\n",
    "        self.weights = self.weights + delta * feature_values\n",
    "\n",
    "\n",
    "    # evaluate q function\n",
    "    def evaluate(self, state, action):\n",
    "        # extract features from state\n",
    "        feature_values = np.array(self.features.extract(state, action))\n",
    "        #print(\"feature values: \",feature_values)\n",
    "        #print(\"weights: \",self.weights)\n",
    "        # compute Q value\n",
    "        Q = np.dot(feature_values, self.weights)\n",
    "        return Q\n",
    "\n",
    "\n",
    "    def evaluate_V(self, state):\n",
    "        return self.V[state]\n",
    "\n",
    "    def update_V_from_Q(self):\n",
    "        for state in self.mdp.states:\n",
    "            actions = self.mdp.get_actions(state)\n",
    "            self.V[state] =  max([self.evaluate(state, action) for action in actions])\n",
    "\n",
    "\n",
    "\n",
    "# defining a feature extractor class for gridworld problem (hand-engineered features)\n",
    "class GridWorldFeatures:\n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "        self.num_features = 3\n",
    "        \n",
    "\n",
    "    def get_num_features(self):\n",
    "        return self.num_features    \n",
    " \n",
    " \n",
    "    def get_num_actions(self):\n",
    "        return len(self.mdp.get_actions())\n",
    "\n",
    "\n",
    "    '''\n",
    "        We will define three (normalized) features:\n",
    "        1) x-distance from goal\n",
    "        2) y-distance from goal\n",
    "        3) manhattan distance from goal\n",
    "    '''\n",
    "    def extract(self, state, action):\n",
    "        (xg, yg) = self.mdp.goal\n",
    "        (x, y) = state\n",
    "        e = 0.01  # small additive value for avoiding division by zero        \n",
    "\n",
    "        feature_values = []\n",
    "        for a in self.mdp.get_actions():\n",
    "            if (a == action) and (state != self.mdp.exit):\n",
    "                feature_values.append((x+e)/(xg+e))\n",
    "                feature_values.append((y+e)/(yg+e))\n",
    "                feature_values.append((abs(xg-x)+abs(yg-y)+e)/(xg+yg+e))\n",
    "            else:\n",
    "                feature_values += [0.0 for _ in range(self.num_features)]\n",
    "        \n",
    "        return feature_values        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate grid world mdp\n",
    "gw = GridWorld(discount_factor=0.9, withQTable=False)\n",
    "\n",
    "# instantiate feature extractor\n",
    "features = GridWorldFeatures(gw)\n",
    "\n",
    "# instantiate linear q function object\n",
    "qfunction = LinearQ(gw, features)\n",
    "\n",
    "# instantiate Q learner\n",
    "QL = QLearner(gw, qfunction, epsilon=0.1, alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode# 0, length: 23, accumulated reward: -0.09847709021836118\n",
      "-----------------------\n",
      "-0.00 -0.00 -0.01 -0.61 \n",
      " 0.00  0.00 -0.01 -0.58 \n",
      " 0.00 -0.00 -0.01 -0.01 \n",
      "-----------------------\n",
      "Episode# 1, length: 327, accumulated reward: -1.2107600349290264e-15\n",
      "-----------------------\n",
      " 0.00 -0.00 -0.00 -0.67 \n",
      " 0.00  0.00 -0.00 -0.63 \n",
      " 0.00  0.00 -0.00 -0.00 \n",
      "-----------------------\n",
      "Episode# 2, length: 174, accumulated reward: -1.2132607080039802e-08\n",
      "-----------------------\n",
      " 0.00 -0.00 -0.00 -0.73 \n",
      " 0.00  0.00 -0.00 -0.68 \n",
      " 0.00 -0.00 -0.00 -0.00 \n",
      "-----------------------\n",
      "Episode# 3, length: 18, accumulated reward: -0.16677181699666577\n",
      "-----------------------\n",
      " 0.00 -0.00 -0.00 -0.78 \n",
      " 0.00  0.00 -0.00 -0.72 \n",
      " 0.00 -0.00 -0.00 -0.00 \n",
      "-----------------------\n",
      "Episode# 4, length: 191, accumulated reward: -2.023376927644849e-09\n",
      "-----------------------\n",
      " 0.00  0.00 -0.00 -0.82 \n",
      " 0.00  0.00 -0.00 -0.76 \n",
      " 0.00  0.00 -0.00 -0.00 \n",
      "-----------------------\n",
      "Episode# 5, length: 70, accumulated reward: -0.0006961986091308868\n",
      "-----------------------\n",
      " 0.00  0.00 -0.00 -0.86 \n",
      " 0.00  0.00 -0.00 -0.79 \n",
      " 0.00  0.00 -0.00 -0.00 \n",
      "-----------------------\n",
      "Episode# 6, length: 38, accumulated reward: -0.020275559590445275\n",
      "-----------------------\n",
      " 0.00 -0.00 -0.00 -0.89 \n",
      " 0.00  0.00 -0.00 -0.82 \n",
      " 0.00  0.00 -0.00 -0.00 \n",
      "-----------------------\n",
      "Episode# 7, length: 124, accumulated reward: -2.354120347120953e-06\n",
      "-----------------------\n",
      " 0.00 -0.00 -0.01 -0.92 \n",
      " 0.00  0.00 -0.00 -0.84 \n",
      " 0.01 -0.00 -0.00 -0.01 \n",
      "-----------------------\n",
      "Episode# 8, length: 4447, accumulated reward: -3.6491941269841135e-204\n",
      "-----------------------\n",
      " 0.01  0.00  0.00 -0.94 \n",
      " 0.01  0.00  0.00 -0.86 \n",
      " 0.01  0.01  0.01  0.01 \n",
      "-----------------------\n",
      "Episode# 9, length: 5734, accumulated reward: 4.7022597503474046e-263\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.55 \n",
      " 0.00  0.00  0.00 -0.57 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 10, length: 15869, accumulated reward: 0.0\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.24 \n",
      " 0.00  0.00  0.00 -0.34 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 11, length: 13132, accumulated reward: 0.0\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.34 \n",
      " 0.00  0.00  0.00 -0.42 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 12, length: 4117, accumulated reward: -4.5937658222007645e-189\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.43 \n",
      " 0.00  0.00  0.00 -0.50 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 13, length: 4564, accumulated reward: -1.6164808783905612e-209\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.50 \n",
      " 0.00  0.00  0.00 -0.56 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 14, length: 14587, accumulated reward: 0.0\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.57 \n",
      " 0.00  0.00  0.00 -0.62 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 15, length: 2781, accumulated reward: -6.225528724961971e-128\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.63 \n",
      " 0.00  0.00  0.00 -0.67 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 16, length: 7531, accumulated reward: 0.0\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.68 \n",
      " 0.00  0.00  0.00 -0.71 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 17, length: 258, accumulated reward: 1.7391014849060135e-12\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.34 \n",
      " 0.00  0.00  0.00 -0.46 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 18, length: 125, accumulated reward: 2.118708312408858e-06\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.07 \n",
      " 0.00  0.00  0.00 -0.26 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 19, length: 580, accumulated reward: -3.209319577213727e-27\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.19 \n",
      " 0.00  0.00  0.00 -0.35 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 20, length: 295, accumulated reward: 3.526125579104374e-14\n",
      "-----------------------\n",
      " 0.00  0.00  0.00  0.05 \n",
      " 0.00  0.00  0.00 -0.17 \n",
      " 0.01  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 21, length: 5267, accumulated reward: -1.0991442884223895e-241\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.07 \n",
      " 0.00  0.00  0.00 -0.28 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 22, length: 4122, accumulated reward: -2.7125727803513296e-189\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.18 \n",
      " 0.00  0.00  0.00 -0.37 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 23, length: 1796, accumulated reward: 7.333384285638083e-83\n",
      "-----------------------\n",
      " 0.00  0.00  0.00  0.06 \n",
      " 0.00  0.00  0.00 -0.20 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 24, length: 1938, accumulated reward: -2.3320657138229184e-89\n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -0.06 \n",
      " 0.00  0.00  0.00 -0.30 \n",
      " 0.01  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 25, length: 14668, accumulated reward: 0.0\n",
      "-----------------------\n",
      " 0.00  0.00  0.00  0.15 \n",
      " 0.00  0.00  0.00 -0.14 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 26, length: 1296, accumulated reward: 5.546892176712517e-60\n",
      "-----------------------\n",
      " 0.02  0.02  0.01  0.32 \n",
      " 0.01  0.00  0.01 -0.01 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 27, length: 40, accumulated reward: -0.016423203268260675\n",
      "-----------------------\n",
      " 0.02  0.01  0.01  0.17 \n",
      " 0.01  0.00  0.00 -0.14 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 28, length: 10, accumulated reward: 0.3874204890000001\n",
      "-----------------------\n",
      " 0.03  0.03  0.03  0.34 \n",
      " 0.02  0.00  0.02 -0.02 \n",
      " 0.00  0.00  0.01  0.01 \n",
      "-----------------------\n",
      "Episode# 29, length: 10, accumulated reward: -0.3874204890000001\n",
      "-----------------------\n",
      " 0.03  0.03  0.03  0.19 \n",
      " 0.02  0.00  0.02 -0.14 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 30, length: 6, accumulated reward: -0.5904900000000001\n",
      "-----------------------\n",
      " 0.03  0.02  0.02  0.06 \n",
      " 0.01  0.00  0.01 -0.25 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 31, length: 15, accumulated reward: 0.2287679245496101\n",
      "-----------------------\n",
      " 0.03  0.03  0.02  0.25 \n",
      " 0.01  0.00  0.01 -0.11 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 32, length: 15, accumulated reward: -0.2287679245496101\n",
      "-----------------------\n",
      " 0.02  0.01  0.01  0.11 \n",
      " 0.00  0.00  0.00 -0.23 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 33, length: 62, accumulated reward: 0.0016173092699229906\n",
      "-----------------------\n",
      " 0.03  0.02  0.02  0.29 \n",
      " 0.01  0.00  0.01 -0.09 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 34, length: 27, accumulated reward: 0.06461081889226677\n",
      "-----------------------\n",
      " 0.05  0.05  0.06  0.43 \n",
      " 0.03  0.00  0.03  0.01 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Episode# 35, length: 76, accumulated reward: -0.00036998848503512764\n",
      "-----------------------\n",
      " 0.04  0.04  0.05  0.28 \n",
      " 0.02  0.00  0.02 -0.12 \n",
      " 0.01  0.01  0.01  0.01 \n",
      "-----------------------\n",
      "Episode# 36, length: 22, accumulated reward: 0.10941898913151242\n",
      "-----------------------\n",
      " 0.06  0.07  0.08  0.42 \n",
      " 0.03  0.00  0.04 -0.01 \n",
      " 0.01  0.01  0.01  0.01 \n",
      "-----------------------\n",
      "Episode# 37, length: 20, accumulated reward: -0.13508517176729928\n",
      "-----------------------\n",
      " 0.05  0.06  0.06  0.28 \n",
      " 0.02  0.00  0.03 -0.14 \n",
      " 0.01  0.01  0.01  0.01 \n",
      "-----------------------\n",
      "Episode# 38, length: 14, accumulated reward: 0.2541865828329001\n",
      "-----------------------\n",
      " 0.07  0.08  0.09  0.42 \n",
      " 0.04  0.00  0.05 -0.03 \n",
      " 0.02  0.01  0.01  0.02 \n",
      "-----------------------\n",
      "Episode# 39, length: 14, accumulated reward: 0.2541865828329001\n",
      "-----------------------\n",
      " 0.11  0.12  0.13  0.54 \n",
      " 0.05  0.00  0.08  0.06 \n",
      " 0.02  0.02  0.03  0.04 \n",
      "-----------------------\n",
      "Episode# 40, length: 9, accumulated reward: 0.4304672100000001\n",
      "-----------------------\n",
      " 0.15  0.17  0.19  0.63 \n",
      " 0.08  0.00  0.12  0.13 \n",
      " 0.02  0.03  0.05  0.07 \n",
      "-----------------------\n",
      "Episode# 41, length: 19, accumulated reward: 0.15009463529699918\n",
      "-----------------------\n",
      " 0.18  0.21  0.23  0.70 \n",
      " 0.10  0.00  0.15  0.18 \n",
      " 0.03  0.04  0.07  0.09 \n",
      "-----------------------\n",
      "Episode# 42, length: 43, accumulated reward: -0.011972515182562033\n",
      "-----------------------\n",
      " 0.15  0.18  0.21  0.53 \n",
      " 0.07  0.00  0.13  0.03 \n",
      " 0.04  0.04  0.05  0.08 \n",
      "-----------------------\n",
      "Episode# 43, length: 7, accumulated reward: 0.531441\n",
      "-----------------------\n",
      " 0.18  0.22  0.25  0.62 \n",
      " 0.09  0.00  0.16  0.10 \n",
      " 0.05  0.04  0.07  0.10 \n",
      "-----------------------\n",
      "Episode# 44, length: 8, accumulated reward: 0.4782969000000001\n",
      "-----------------------\n",
      " 0.22  0.26  0.30  0.70 \n",
      " 0.12  0.00  0.20  0.16 \n",
      " 0.06  0.05  0.09  0.13 \n",
      "-----------------------\n",
      "Episode# 45, length: 8, accumulated reward: 0.4782969000000001\n",
      "-----------------------\n",
      " 0.27  0.31  0.35  0.76 \n",
      " 0.15  0.00  0.23  0.20 \n",
      " 0.06  0.07  0.11  0.15 \n",
      "-----------------------\n",
      "Episode# 46, length: 19, accumulated reward: -0.15009463529699918\n",
      "-----------------------\n",
      " 0.27  0.31  0.35  0.58 \n",
      " 0.15  0.00  0.23  0.05 \n",
      " 0.07  0.07  0.11  0.15 \n",
      "-----------------------\n",
      "Episode# 47, length: 32, accumulated reward: 0.038152042447694615\n",
      "-----------------------\n",
      " 0.26  0.31  0.35  0.66 \n",
      " 0.13  0.00  0.23  0.11 \n",
      " 0.10  0.09  0.11  0.15 \n",
      "-----------------------\n",
      "Episode# 48, length: 13, accumulated reward: 0.2824295364810001\n",
      "-----------------------\n",
      " 0.29  0.34  0.39  0.73 \n",
      " 0.15  0.00  0.26  0.16 \n",
      " 0.10  0.09  0.12  0.17 \n",
      "-----------------------\n",
      "Episode# 49, length: 20, accumulated reward: 0.13508517176729928\n",
      "-----------------------\n",
      " 0.31  0.37  0.42  0.78 \n",
      " 0.16  0.00  0.28  0.20 \n",
      " 0.11  0.09  0.14  0.19 \n",
      "-----------------------\n",
      "Episode# 50, length: 22, accumulated reward: -0.10941898913151242\n",
      "-----------------------\n",
      " 0.30  0.36  0.41  0.60 \n",
      " 0.16  0.00  0.27  0.05 \n",
      " 0.10  0.09  0.13  0.19 \n",
      "-----------------------\n",
      "Episode# 51, length: 7, accumulated reward: 0.531441\n",
      "-----------------------\n",
      " 0.33  0.38  0.44  0.68 \n",
      " 0.18  0.00  0.29  0.11 \n",
      " 0.11  0.09  0.14  0.20 \n",
      "-----------------------\n",
      "Episode# 52, length: 18, accumulated reward: 0.16677181699666577\n",
      "-----------------------\n",
      " 0.34  0.40  0.46  0.75 \n",
      " 0.18  0.00  0.30  0.16 \n",
      " 0.12  0.11  0.15  0.21 \n",
      "-----------------------\n",
      "Episode# 53, length: 9, accumulated reward: 0.4304672100000001\n",
      "-----------------------\n",
      " 0.37  0.44  0.50  0.80 \n",
      " 0.21  0.00  0.33  0.19 \n",
      " 0.13  0.11  0.17  0.23 \n",
      "-----------------------\n",
      "Episode# 54, length: 33, accumulated reward: 0.03433683820292515\n",
      "-----------------------\n",
      " 0.37  0.45  0.52  0.84 \n",
      " 0.20  0.00  0.34  0.22 \n",
      " 0.15  0.13  0.16  0.23 \n",
      "-----------------------\n",
      "Episode# 55, length: 23, accumulated reward: 0.09847709021836118\n",
      "-----------------------\n",
      " 0.39  0.47  0.55  0.87 \n",
      " 0.21  0.00  0.36  0.25 \n",
      " 0.16  0.14  0.17  0.25 \n",
      "-----------------------\n",
      "Episode# 56, length: 23, accumulated reward: 0.09847709021836118\n",
      "-----------------------\n",
      " 0.36  0.45  0.53  0.90 \n",
      " 0.21  0.00  0.35  0.27 \n",
      " 0.19  0.16  0.17  0.25 \n",
      "-----------------------\n",
      "Episode# 57, length: 12, accumulated reward: 0.31381059609000006\n",
      "-----------------------\n",
      " 0.37  0.46  0.55  0.92 \n",
      " 0.22  0.00  0.36  0.28 \n",
      " 0.21  0.18  0.18  0.27 \n",
      "-----------------------\n",
      "Episode# 58, length: 6, accumulated reward: 0.5904900000000001\n",
      "-----------------------\n",
      " 0.41  0.50  0.60  0.93 \n",
      " 0.23  0.00  0.40  0.30 \n",
      " 0.21  0.18  0.19  0.29 \n",
      "-----------------------\n",
      "Episode# 59, length: 9, accumulated reward: 0.4304672100000001\n",
      "-----------------------\n",
      " 0.44  0.54  0.64  0.95 \n",
      " 0.24  0.00  0.42  0.31 \n",
      " 0.22  0.19  0.21  0.31 \n",
      "-----------------------\n",
      "Episode# 60, length: 6, accumulated reward: 0.5904900000000001\n",
      "-----------------------\n",
      " 0.47  0.58  0.68  0.96 \n",
      " 0.26  0.00  0.45  0.31 \n",
      " 0.23  0.20  0.22  0.33 \n",
      "-----------------------\n",
      "Episode# 61, length: 6, accumulated reward: 0.5904900000000001\n",
      "-----------------------\n",
      " 0.50  0.61  0.72  0.97 \n",
      " 0.27  0.00  0.48  0.32 \n",
      " 0.25  0.21  0.24  0.34 \n",
      "-----------------------\n",
      "Episode# 62, length: 7, accumulated reward: 0.531441\n",
      "-----------------------\n",
      " 0.52  0.63  0.74  0.97 \n",
      " 0.29  0.00  0.49  0.33 \n",
      " 0.26  0.22  0.25  0.36 \n",
      "-----------------------\n",
      "Episode# 63, length: 6, accumulated reward: 0.5904900000000001\n",
      "-----------------------\n",
      " 0.55  0.66  0.77  0.98 \n",
      " 0.29  0.00  0.51  0.33 \n",
      " 0.26  0.22  0.26  0.37 \n",
      "-----------------------\n",
      "Episode# 64, length: 11, accumulated reward: 0.3486784401000001\n",
      "-----------------------\n",
      " 0.56  0.67  0.79  0.98 \n",
      " 0.30  0.00  0.53  0.33 \n",
      " 0.28  0.23  0.26  0.38 \n",
      "-----------------------\n",
      "Episode# 65, length: 8, accumulated reward: 0.4782969000000001\n",
      "-----------------------\n",
      " 0.58  0.70  0.81  0.99 \n",
      " 0.32  0.00  0.54  0.34 \n",
      " 0.29  0.24  0.27  0.39 \n",
      "-----------------------\n",
      "Episode# 66, length: 11, accumulated reward: -0.3486784401000001\n",
      "-----------------------\n",
      " 0.50  0.61  0.72  0.79 \n",
      " 0.37  0.00  0.47  0.16 \n",
      " 0.33  0.27  0.23  0.34 \n",
      "-----------------------\n",
      "Episode# 67, length: 7, accumulated reward: 0.531441\n",
      "-----------------------\n",
      " 0.51  0.62  0.73  0.83 \n",
      " 0.37  0.00  0.48  0.20 \n",
      " 0.34  0.28  0.23  0.34 \n",
      "-----------------------\n",
      "Episode# 68, length: 10, accumulated reward: 0.3874204890000001\n",
      "-----------------------\n",
      " 0.51  0.62  0.73  0.86 \n",
      " 0.38  0.00  0.48  0.22 \n",
      " 0.35  0.29  0.23  0.34 \n",
      "-----------------------\n",
      "Episode# 69, length: 6, accumulated reward: 0.5904900000000001\n",
      "-----------------------\n",
      " 0.53  0.64  0.75  0.89 \n",
      " 0.39  0.00  0.49  0.24 \n",
      " 0.35  0.29  0.24  0.35 \n",
      "-----------------------\n",
      "Episode# 70, length: 8, accumulated reward: 0.4782969000000001\n",
      "-----------------------\n",
      " 0.55  0.66  0.77  0.91 \n",
      " 0.40  0.00  0.51  0.26 \n",
      " 0.36  0.30  0.25  0.36 \n",
      "-----------------------\n",
      "Episode# 71, length: 6, accumulated reward: 0.5904900000000001\n",
      "-----------------------\n",
      " 0.56  0.67  0.78  0.93 \n",
      " 0.41  0.00  0.52  0.27 \n",
      " 0.37  0.30  0.25  0.36 \n",
      "-----------------------\n",
      "Episode# 72, length: 10, accumulated reward: -0.3874204890000001\n",
      "-----------------------\n",
      " 0.52  0.62  0.72  0.74 \n",
      " 0.41  0.00  0.47  0.11 \n",
      " 0.36  0.30  0.24  0.32 \n",
      "-----------------------\n",
      "Episode# 73, length: 8, accumulated reward: 0.4782969000000001\n",
      "-----------------------\n",
      " 0.52  0.62  0.72  0.79 \n",
      " 0.41  0.00  0.47  0.15 \n",
      " 0.36  0.30  0.24  0.32 \n",
      "-----------------------\n",
      "Episode# 74, length: 14, accumulated reward: 0.2541865828329001\n",
      "-----------------------\n",
      " 0.51  0.61  0.70  0.83 \n",
      " 0.41  0.00  0.46  0.18 \n",
      " 0.36  0.30  0.24  0.31 \n",
      "-----------------------\n",
      "Episode# 75, length: 8, accumulated reward: 0.4782969000000001\n",
      "-----------------------\n",
      " 0.51  0.61  0.71  0.87 \n",
      " 0.41  0.00  0.46  0.20 \n",
      " 0.37  0.31  0.24  0.32 \n",
      "-----------------------\n",
      "Episode# 76, length: 6, accumulated reward: 0.5904900000000001\n",
      "-----------------------\n",
      " 0.53  0.63  0.73  0.89 \n",
      " 0.42  0.00  0.48  0.22 \n",
      " 0.37  0.31  0.25  0.32 \n",
      "-----------------------\n",
      "Episode# 77, length: 8, accumulated reward: -0.4782969000000001\n",
      "-----------------------\n",
      " 0.48  0.57  0.66  0.71 \n",
      " 0.42  0.00  0.43  0.07 \n",
      " 0.37  0.31  0.25  0.28 \n",
      "-----------------------\n",
      "Episode# 78, length: 9, accumulated reward: 0.4304672100000001\n",
      "-----------------------\n",
      " 0.49  0.58  0.67  0.77 \n",
      " 0.41  0.00  0.43  0.11 \n",
      " 0.37  0.30  0.24  0.28 \n",
      "-----------------------\n",
      "Episode# 79, length: 7, accumulated reward: 0.531441\n",
      "-----------------------\n",
      " 0.50  0.59  0.68  0.81 \n",
      " 0.41  0.00  0.44  0.14 \n",
      " 0.37  0.30  0.24  0.28 \n",
      "-----------------------\n",
      "Episode# 80, length: 9, accumulated reward: 0.4304672100000001\n",
      "-----------------------\n",
      " 0.49  0.58  0.67  0.85 \n",
      " 0.42  0.00  0.43  0.17 \n",
      " 0.37  0.31  0.24  0.29 \n",
      "-----------------------\n",
      "Episode# 81, length: 8, accumulated reward: 0.4782969000000001\n",
      "-----------------------\n",
      " 0.51  0.60  0.69  0.88 \n",
      " 0.42  0.00  0.45  0.19 \n",
      " 0.37  0.31  0.24  0.29 \n",
      "-----------------------\n",
      "Episode# 82, length: 7, accumulated reward: 0.531441\n",
      "-----------------------\n",
      " 0.52  0.61  0.71  0.90 \n",
      " 0.42  0.00  0.46  0.21 \n",
      " 0.37  0.31  0.25  0.30 \n",
      "-----------------------\n",
      "Episode# 83, length: 10, accumulated reward: 0.3874204890000001\n",
      "-----------------------\n",
      " 0.52  0.62  0.72  0.92 \n",
      " 0.43  0.00  0.46  0.23 \n",
      " 0.38  0.32  0.25  0.31 \n",
      "-----------------------\n",
      "Episode# 84, length: 6, accumulated reward: 0.5904900000000001\n",
      "-----------------------\n",
      " 0.54  0.64  0.74  0.94 \n",
      " 0.44  0.00  0.48  0.24 \n",
      " 0.39  0.32  0.25  0.32 \n",
      "-----------------------\n",
      "Episode# 85, length: 8, accumulated reward: -0.4782969000000001\n",
      "-----------------------\n",
      " 0.54  0.63  0.73  0.75 \n",
      " 0.44  0.00  0.47  0.08 \n",
      " 0.39  0.32  0.26  0.30 \n",
      "-----------------------\n",
      "Episode# 86, length: 8, accumulated reward: -0.4782969000000001\n",
      "-----------------------\n",
      " 0.51  0.60  0.68  0.59 \n",
      " 0.43  0.00  0.43 -0.06 \n",
      " 0.38  0.32  0.25  0.27 \n",
      "-----------------------\n",
      "Episode# 87, length: 6, accumulated reward: 0.5904900000000001\n",
      "-----------------------\n",
      " 0.50  0.58  0.67  0.67 \n",
      " 0.43  0.00  0.42  0.00 \n",
      " 0.38  0.32  0.25  0.26 \n",
      "-----------------------\n",
      "Episode# 88, length: 8, accumulated reward: -0.4782969000000001\n",
      "-----------------------\n",
      " 0.49  0.51  0.59  0.52 \n",
      " 0.43  0.00  0.36 -0.13 \n",
      " 0.38  0.32  0.25  0.21 \n",
      "-----------------------\n",
      "Episode# 89, length: 12, accumulated reward: -0.31381059609000006\n",
      "-----------------------\n",
      " 0.47  0.44  0.50  0.39 \n",
      " 0.42  0.00  0.30 -0.24 \n",
      " 0.37  0.31  0.25  0.19 \n",
      "-----------------------\n",
      "Episode# 90, length: 20, accumulated reward: 0.13508517176729928\n",
      "-----------------------\n",
      " 0.40  0.41  0.47  0.51 \n",
      " 0.37  0.00  0.27 -0.15 \n",
      " 0.34  0.29  0.23  0.18 \n",
      "-----------------------\n",
      "Episode# 91, length: 18, accumulated reward: -0.16677181699666577\n",
      "-----------------------\n",
      " 0.35  0.35  0.40  0.39 \n",
      " 0.33  0.00  0.23 -0.26 \n",
      " 0.32  0.27  0.22  0.17 \n",
      "-----------------------\n",
      "Episode# 92, length: 6, accumulated reward: 0.5904900000000001\n",
      "-----------------------\n",
      " 0.34  0.35  0.39  0.51 \n",
      " 0.33  0.00  0.23 -0.17 \n",
      " 0.31  0.26  0.21  0.16 \n",
      "-----------------------\n",
      "Episode# 93, length: 11, accumulated reward: 0.3486784401000001\n",
      "-----------------------\n",
      " 0.32  0.37  0.41  0.61 \n",
      " 0.31  0.00  0.23 -0.09 \n",
      " 0.30  0.25  0.21  0.16 \n",
      "-----------------------\n",
      "Episode# 94, length: 10, accumulated reward: 0.3874204890000001\n",
      "-----------------------\n",
      " 0.33  0.38  0.43  0.69 \n",
      " 0.30  0.00  0.24 -0.03 \n",
      " 0.29  0.25  0.20  0.16 \n",
      "-----------------------\n",
      "Episode# 95, length: 7, accumulated reward: -0.531441\n",
      "-----------------------\n",
      " 0.31  0.33  0.37  0.54 \n",
      " 0.30  0.00  0.21 -0.16 \n",
      " 0.29  0.24  0.20  0.15 \n",
      "-----------------------\n",
      "Episode# 96, length: 13, accumulated reward: 0.2824295364810001\n",
      "-----------------------\n",
      " 0.31  0.35  0.39  0.63 \n",
      " 0.29  0.00  0.21 -0.09 \n",
      " 0.28  0.24  0.19  0.15 \n",
      "-----------------------\n",
      "Episode# 97, length: 8, accumulated reward: 0.4782969000000001\n",
      "-----------------------\n",
      " 0.32  0.37  0.42  0.71 \n",
      " 0.28  0.00  0.23 -0.04 \n",
      " 0.27  0.23  0.19  0.15 \n",
      "-----------------------\n",
      "Episode# 98, length: 7, accumulated reward: 0.531441\n",
      "-----------------------\n",
      " 0.35  0.40  0.45  0.76 \n",
      " 0.29  0.00  0.25  0.01 \n",
      " 0.28  0.24  0.20  0.15 \n",
      "-----------------------\n",
      "Episode# 99, length: 6, accumulated reward: 0.5904900000000001\n",
      "-----------------------\n",
      " 0.38  0.43  0.49  0.81 \n",
      " 0.29  0.00  0.28  0.04 \n",
      " 0.28  0.24  0.20  0.15 \n",
      "-----------------------\n",
      "-----------------------\n",
      "right  right  right  end    \n",
      "up     None   right  end    \n",
      "up     up     up     up     \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "episode_rewards = QL.train(episodes=100)\n",
    "\n",
    "# policy extraction\n",
    "pi = qfunction.extract_policy(gw)\n",
    "\n",
    "print(\"-----------------------\")\n",
    "for y in range(gw.height-1, -1, -1):\n",
    "    for x in range(gw.width):\n",
    "        if (x,y) in pi:\n",
    "            print(f\"{pi[(x,y)]:<6}\", end=' ')\n",
    "        else:\n",
    "            print(f\"{'None':<6}\", end=' ')\n",
    "    print(\"\")       \n",
    "print(\"-----------------------\")\n",
    "\n",
    "#plot_rewards(episode_rewards, window_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
