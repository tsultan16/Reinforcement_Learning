{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld using Value Iteration: In this example, we will use value iteration to obtain the Q/value -function for the GridWorld problem and extract a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, define an interface for an MDP\n",
    "class MDP:\n",
    "    @abstractmethod\n",
    "    def get_states(self):\n",
    "        pass   \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_initial_state(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_terminal_states(self):\n",
    "        pass    \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_transitions(self, state, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_rewards(self, state, action, next_state):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_terminal(self, state):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_discount_factor(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "# now, implement the gridworld problem MDP\n",
    "class GridWorld(MDP):\n",
    "\n",
    "    def __init__(self, discount_factor=0.9) :\n",
    "\n",
    "        # initialise the set of all possible states, in this case tuples (x,y) of all grid cells, excluding walls\n",
    "        # will use the example from lectures and tute week 7\n",
    "        self.width = 4\n",
    "        self.height = 3\n",
    "        self.walls = [(1,1)]\n",
    "        self.states = []\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                if (x,y) not in self.walls:\n",
    "                   self.states.append((x,y))\n",
    "\n",
    "        print(f\"States: {self.states}\")\n",
    "\n",
    "        # specify terminal states\n",
    "        self.terminal_states=[(3,1),(3,2)] \n",
    "\n",
    "        # create a dummy terminal state which is the successor to all terminal states\n",
    "        self.exit = (-1,-1)\n",
    "\n",
    "        # specify initial state\n",
    "        self.initial_state = (0,0)\n",
    "\n",
    "        # specify probability of splipping\n",
    "        self.noise = 0.2\n",
    "\n",
    "        # specify/enumerate the actions\n",
    "        self.terminate = 0\n",
    "        self.up = 1\n",
    "        self.down = 2\n",
    "        self.left = 3\n",
    "        self.right = 4\n",
    "        \n",
    "        # set the discount factor\n",
    "        self.gamma = discount_factor\n",
    "\n",
    "        # specify rewards\n",
    "        self.rewards =  {(3,1) : -1, (3,2) : 1}\n",
    "\n",
    "        # empty list for storing the discounted reward at each step opf the episode\n",
    "        self.episode_discounted_rewards = []\n",
    "\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states   \n",
    "\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return self.initial_state\n",
    "\n",
    "\n",
    "    def get_terminal_states(self):\n",
    "        return self.terminal_states\n",
    "\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        if state not in self.terminal_states:\n",
    "            actions = [self.up, self.down, self.left, self.right]\n",
    "        else:\n",
    "            # for terminal states, the only valid action is to 'terminate' the episode\n",
    "            actions = [self.terminate]\n",
    "        return actions        \n",
    "\n",
    "    \n",
    "    # for given state-action pair, returns possible successor states along with their corresponding transition probabilities\n",
    "    def get_transitions(self, state, action):\n",
    "        \n",
    "        # if we're in an terminal state, then the only allowed action is to transition into the 'exit' state which terminates the episode\n",
    "        if state in self.terminal_states:\n",
    "            if action == self.terminate:\n",
    "                return [(self.exit, 1.0)]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        # probability of not slipping\n",
    "        straight = 1 - 2*self.noise\n",
    "\n",
    "        transitions = []\n",
    "        (x,y) = state\n",
    "\n",
    "        if action == self.up:\n",
    "            transitions.append(self.valid_add(state, (x,y+1), straight))\n",
    "            transitions.append(self.valid_add(state, (x-1,y), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x+1,y), self.noise))\n",
    "\n",
    "        elif action == self.down:\n",
    "            transitions.append(self.valid_add(state, (x,y-1), straight))\n",
    "            transitions.append(self.valid_add(state, (x-1,y), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x+1,y), self.noise))\n",
    "\n",
    "        elif action == self.left:\n",
    "            transitions.append(self.valid_add(state, (x-1,y), straight))\n",
    "            transitions.append(self.valid_add(state, (x,y-1), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x,y+1), self.noise))\n",
    "\n",
    "        elif action == self.right:\n",
    "            transitions.append(self.valid_add(state, (x+1,y), straight))\n",
    "            transitions.append(self.valid_add(state, (x,y-1), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x,y+1), self.noise))\n",
    "\n",
    "        # convert list to set to remove duplicates\n",
    "        return set(transitions)\n",
    "    \n",
    "\n",
    "    def get_rewards(self, state, action, next_state):\n",
    "        if (next_state in self.terminal_states):\n",
    "            reward = self.rewards[next_state]\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        # store discounted reward for the step\n",
    "        step = len(self.episode_discounted_rewards)\n",
    "        self.episode_discounted_rewards.append(reward * (self.gamma**step))\n",
    "\n",
    "        return reward    \n",
    "\n",
    "\n",
    "    def is_exit(self, state):\n",
    "        return (state == self.exit)    \n",
    "\n",
    "\n",
    "    def get_discount_factor(self):\n",
    "        return self.gamma  \n",
    "\n",
    "\n",
    "    def valid_add(self, state, next_state, probability):\n",
    "\n",
    "        # check if next state is a wall\n",
    "        if next_state in self.walls:\n",
    "            return (state, probability)\n",
    "\n",
    "        # check if next state is off grid\n",
    "        (x,y) = next_state\n",
    "        if (x >=0 and x<=3 and y>=0 and y<=2):\n",
    "            return (next_state, probability)\n",
    "        else:\n",
    "            return (state, probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_planning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
