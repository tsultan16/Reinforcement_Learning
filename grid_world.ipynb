{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld using Value Iteration: In this example, we will use value iteration to obtain the Q/value -function for the GridWorld problem and extract a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, define an interface for an MDP\n",
    "class MDP:\n",
    "    @abstractmethod\n",
    "    def get_states(self):\n",
    "        pass   \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_initial_state(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_terminal_states(self):\n",
    "        pass    \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_transitions(self, state, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_rewards(self, state, action, next_state):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_terminal(self, state):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_discount_factor(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "# now, implement the gridworld problem MDP\n",
    "class GridWorld(MDP):\n",
    "\n",
    "    def __init__(self, discount_factor=0.9) :\n",
    "\n",
    "        # initialise the set of all possible states, in this case tuples (x,y) of all grid cells, excluding walls\n",
    "        # will use the example from lectures and tute week 7\n",
    "        self.width = 4\n",
    "        self.height = 3\n",
    "        self.walls = [(1,1)]\n",
    "        self.states = []\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                if (x,y) not in self.walls:\n",
    "                   self.states.append((x,y))\n",
    "\n",
    "        #print(f\"States: {self.states}\")\n",
    "\n",
    "        # specify terminal states\n",
    "        self.terminal_states=[(3,1),(3,2)] \n",
    "\n",
    "        # create a dummy terminal state which is the successor to all terminal states\n",
    "        self.exit = (-1,-1)\n",
    "\n",
    "        # specify initial state\n",
    "        self.initial_state = (0,0)\n",
    "\n",
    "        # specify probability of splipping\n",
    "        self.noise = 0.2\n",
    "\n",
    "        # specify/enumerate the actions\n",
    "        self.terminate = 0\n",
    "        self.up = 1\n",
    "        self.down = 2\n",
    "        self.left = 3\n",
    "        self.right = 4\n",
    "        \n",
    "        # set the discount factor\n",
    "        self.gamma = discount_factor\n",
    "\n",
    "        # specify rewards\n",
    "        self.rewards =  {(3,1) : -1, (3,2) : 1}\n",
    "\n",
    "        # empty list for storing the discounted reward at each step opf the episode\n",
    "        self.episode_discounted_rewards = []\n",
    "\n",
    "        # initialize Q-function and value function\n",
    "        self.Q = {}\n",
    "        self.V = {}\n",
    "        self.Vtemp = {}\n",
    "        for state in (self.states + [self.exit]):\n",
    "            self.Q[(state, self.up)] = 0.0\n",
    "            self.Q[(state, self.down)] = 0.0\n",
    "            self.Q[(state, self.left)] = 0.0\n",
    "            self.Q[(state, self.right)] = 0.0\n",
    "            self.V[state] = 0.0\n",
    "            self.Vtemp[state] = 0.0\n",
    "\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states   \n",
    "\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return self.initial_state\n",
    "\n",
    "\n",
    "    def get_terminal_states(self):\n",
    "        return self.terminal_states\n",
    "\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        if state not in self.terminal_states:\n",
    "            actions = [self.up, self.down, self.left, self.right]\n",
    "        else:\n",
    "            # for terminal states, the only valid action is to 'terminate' the episode\n",
    "            actions = [self.terminate]\n",
    "        return actions        \n",
    "\n",
    "    \n",
    "    # for given state-action pair, returns possible successor states along with their corresponding transition probabilities\n",
    "    def get_transitions(self, state, action):\n",
    "        \n",
    "        # if we're in an terminal state, then the only allowed action is to transition into the 'exit' state which terminates the episode\n",
    "        if state in self.terminal_states:\n",
    "            if action == self.terminate:\n",
    "                return [(self.exit, 1.0)]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        # probability of not slipping\n",
    "        straight = 1 - 2*self.noise\n",
    "\n",
    "        transitions = []\n",
    "        (x,y) = state\n",
    "\n",
    "        if action == self.up:\n",
    "            transitions.append(self.valid_add(state, (x,y+1), straight))\n",
    "            transitions.append(self.valid_add(state, (x-1,y), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x+1,y), self.noise))\n",
    "\n",
    "        elif action == self.down:\n",
    "            transitions.append(self.valid_add(state, (x,y-1), straight))\n",
    "            transitions.append(self.valid_add(state, (x-1,y), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x+1,y), self.noise))\n",
    "\n",
    "        elif action == self.left:\n",
    "            transitions.append(self.valid_add(state, (x-1,y), straight))\n",
    "            transitions.append(self.valid_add(state, (x,y-1), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x,y+1), self.noise))\n",
    "\n",
    "        elif action == self.right:\n",
    "            transitions.append(self.valid_add(state, (x+1,y), straight))\n",
    "            transitions.append(self.valid_add(state, (x,y-1), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x,y+1), self.noise))\n",
    "\n",
    "        # convert list to set to remove duplicates\n",
    "        return set(transitions)\n",
    "    \n",
    "\n",
    "    def get_rewards(self, state, action, next_state):\n",
    "        if (next_state in self.terminal_states):\n",
    "            reward = self.rewards[next_state]\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        # store discounted reward for the step\n",
    "        step = len(self.episode_discounted_rewards)\n",
    "        self.episode_discounted_rewards.append(reward * (self.gamma**step))\n",
    "\n",
    "        return reward    \n",
    "\n",
    "\n",
    "    def is_exit(self, state):\n",
    "        return (state == self.exit)    \n",
    "\n",
    "\n",
    "    def get_discount_factor(self):\n",
    "        return self.gamma  \n",
    "\n",
    "\n",
    "    def valid_add(self, state, next_state, probability):\n",
    "\n",
    "        # check if next state is a wall\n",
    "        if next_state in self.walls:\n",
    "            return (state, probability)\n",
    "\n",
    "        # check if next state is off grid\n",
    "        (x,y) = next_state\n",
    "        if (x >=0 and x<=3 and y>=0 and y<=2):\n",
    "            return (next_state, probability)\n",
    "        else:\n",
    "            return (state, probability)\n",
    "\n",
    "\n",
    "    def update_Q(self, state, action, Qnew):\n",
    "        self.Q[(state, action)] = Qnew\n",
    "    \n",
    "    \n",
    "    def update_V(self):\n",
    "        # copy from Vtemp\n",
    "        for state in self.states:\n",
    "            self.V[state] = self.Vtemp[state]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the value iteration loop\n",
    "\n",
    "theta = 0.01 # threshold\n",
    "\n",
    "def value_iteration(grid_world_mdp, num_iters):\n",
    "    states = grid_world_mdp.get_states()    \n",
    "    gamma = grid_world_mdp.get_discount_factor()\n",
    "    for i in range(num_iters):\n",
    "        print(f\"Iteration# {i}\")\n",
    "        d = 0.0 \n",
    "        for state in states:\n",
    "            #print(f\"state: {state}\")\n",
    "            actions = grid_world_mdp.get_actions(state)\n",
    "            Qsa = [] \n",
    "            for action in actions:\n",
    "                #print(f\"action: {action}\")\n",
    "                # get the transitions\n",
    "                transitions = grid_world_mdp.get_transitions(state, action)\n",
    "                #print(f\"transitions: {transitions}\")\n",
    "                # compute Q value\n",
    "                Qnew = 0.0\n",
    "                for tr in transitions:\n",
    "                    (next_state, p) = tr\n",
    "                    Qnew += p * (grid_world_mdp.get_rewards(state, action, next_state) + gamma * grid_world_mdp.V[next_state])\n",
    "                grid_world_mdp.update_Q(state, action, Qnew)\n",
    "                Qsa.append(Qnew)\n",
    "            Vnew = max(Qsa)    \n",
    "            grid_world_mdp.Vtemp[state] = Vnew  \n",
    "            # update delta        \n",
    "            d = max(d, abs(Vnew - grid_world_mdp.V[state]))\n",
    "        \n",
    "        # update the value function\n",
    "        grid_world_mdp.update_V()\n",
    "\n",
    "        # diagnostic updated Q values in cell (0,0)\n",
    "        print(f\"Cell(0,0): Qup = {grid_world_mdp.Q[((0,0),1)]}, Qdown = {grid_world_mdp.Q[((0,0),2)]}, Qleft = {grid_world_mdp.Q[((0,0),3)]}, Qright = {grid_world_mdp.Q[((0,0),4)]}\")\n",
    "        print(f\"Cell(2,2): Qup = {grid_world_mdp.Q[((2,2),1)]}, Qdown = {grid_world_mdp.Q[((2,2),2)]}, Qleft = {grid_world_mdp.Q[((2,2),3)]}, Qright = {grid_world_mdp.Q[((2,2),4)]}\")\n",
    "\n",
    "        # stop value iteration if delta falls below threshold    \n",
    "        if d <= theta:\n",
    "            break    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration# 0\n",
      "Cell(0,0): Qup = 0.0, Qdown = 0.0, Qleft = 0.0, Qright = 0.0\n",
      "Cell(2,2): Qup = 0.2, Qdown = 0.2, Qleft = 0.0, Qright = 0.6\n",
      "Iteration# 1\n",
      "Cell(0,0): Qup = 0.0, Qdown = 0.0, Qleft = 0.0, Qright = 0.0\n",
      "Cell(2,2): Qup = 0.524, Qdown = 0.2, Qleft = 0.10800000000000001, Qright = 0.708\n",
      "Iteration# 2\n",
      "Cell(0,0): Qup = 0.0, Qdown = 0.0, Qleft = 0.0, Qright = 0.0\n",
      "Cell(2,2): Qup = 0.64064, Qdown = 0.32528, Qleft = 0.32472, Qright = 0.74976\n",
      "Iteration# 3\n",
      "Cell(0,0): Qup = 0.0, Qdown = 0.0, Qleft = 0.0, Qright = 0.0\n",
      "Cell(2,2): Qup = 0.6841856000000001, Qdown = 0.3898208, Qleft = 0.40973760000000004, Qright = 0.7717919999999999\n",
      "Iteration# 4\n",
      "Cell(0,0): Qup = 0.057526848000000005, Qdown = 0.006508512, Qleft = 0.017006112, Qright = 0.036531648\n",
      "Cell(2,2): Qup = 0.703921088, Qdown = 0.426211616, Qleft = 0.44673552000000005, Qright = 0.785275296\n",
      "Iteration# 5\n",
      "Cell(0,0): Qup = 0.11001904704000001, Qdown = 0.053332007040000004, Qleft = 0.07066984320000001, Qright = 0.07534337472\n",
      "Cell(2,2): Qup = 0.7147544556800001, Qdown = 0.45155655488, Qleft = 0.46708386047999995, Qright = 0.79496647296\n",
      "Iteration# 6\n",
      "Cell(0,0): Qup = 0.1474460143488, Qdown = 0.095717956608, Qleft = 0.11625982824960002, Qright = 0.1063622710656\n",
      "Cell(2,2): Qup = 0.7219376974208, Qdown = 0.4702039316480001, Qleft = 0.4802440810752, Qright = 0.802276675008\n",
      "Iteration# 7\n",
      "Cell(0,0): Qup = 0.1718490292416, Qdown = 0.12642434360064, Qleft = 0.14784297479424, Qright = 0.1290117668544\n",
      "Cell(2,2): Qup = 0.727178190040064, Qdown = 0.484387966281728, Qleft = 0.48973588502399995, Qright = 0.807889528416768\n",
      "Iteration# 8\n",
      "Cell(0,0): Qup = 0.18754069666996226, Qdown = 0.1470657201954509, Qleft = 0.16815578514227714, Qright = 0.1453605667763098\n",
      "Cell(2,2): Qup = 0.7311524195522663, Qdown = 0.49539756537380863, Qleft = 0.49693150145885184, Qright = 0.8122552788372173\n",
      "Iteration# 9\n",
      "Cell(0,0): Qup = 0.19785996799090605, Qdown = 0.1608641532269983, Qleft = 0.18111856525760198, Qright = 0.15735114392969873\n",
      "Cell(2,2): Qup = 0.7342252860915053, Qdown = 0.5040419190906359, Qleft = 0.5025064179393322, Qright = 0.8156841113811084\n",
      "Iteration# 10\n",
      "Cell(0,0): Qup = 0.20491543401023807, Qdown = 0.17033570575444085, Qleft = 0.1896005472770812, Qright = 0.1663857509649574\n",
      "Cell(2,2): Qup = 0.7366299716422695, Qdown = 0.5108926412364675, Qleft = 0.5068821577846778, Qright = 0.818400503295265\n",
      "Iteration# 11\n",
      "Cell(0,0): Qup = 0.21003512867522242, Qdown = 0.17717964564427724, Qleft = 0.195375718286196, Qright = 0.17364298339138495\n",
      "Cell(2,2): Qup = 0.7385296666750516, Qdown = 0.5163628529058505, Qleft = 0.5103487612833872, Qright = 0.8205685765965617\n",
      "Iteration# 12\n",
      "Cell(0,0): Qup = 0.21393397812158513, Qdown = 0.18240388050344847, Qleft = 0.19954164834707905, Qright = 0.17965844243432394\n",
      "Cell(2,2): Qup = 0.7400423713636526, Qdown = 0.5207596959192221, Qleft = 0.5131164824311465, Qright = 0.8223104624266186\n"
     ]
    }
   ],
   "source": [
    "# instantiate grid world mdp object\n",
    "gw = GridWorld()\n",
    "\n",
    "value_iteration(gw, num_iters=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_planning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
