{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld using Value Iteration: In this example, we will use value iteration to obtain the Q/value -function for the GridWorld problem and extract a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, define an interface for an MDP\n",
    "class MDP:\n",
    "    @abstractmethod\n",
    "    def get_states(self):\n",
    "        pass   \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_initial_state(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_terminal_states(self):\n",
    "        pass    \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_transitions(self, state, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_rewards(self, state, action, next_state):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_terminal(self, state):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_discount_factor(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# now, implement the gridworld problem MDP\n",
    "class GridWorld(MDP):\n",
    "\n",
    "    def __init__(self, discount_factor=0.9) :\n",
    "\n",
    "        # initialise the set of all possible states, in this case tuples (x,y) of all grid cells, excluding walls\n",
    "        # will use the example from lectures and tute week 7\n",
    "        self.width = 4\n",
    "        self.height = 3\n",
    "        self.walls = [(1,1)]\n",
    "        self.states = []\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                if (x,y) not in self.walls:\n",
    "                   self.states.append((x,y))\n",
    "\n",
    "        #print(f\"States: {self.states}\")\n",
    "\n",
    "        # specify terminal states\n",
    "        self.terminal_states=[(3,1),(3,2)] \n",
    "\n",
    "        # create a dummy terminal state which is the successor to all terminal states\n",
    "        self.exit = (-1,-1)\n",
    "\n",
    "        # specify initial state\n",
    "        self.initial_state = (0,0)\n",
    "\n",
    "        # specify probability of splipping\n",
    "        self.noise = 0.1\n",
    "\n",
    "        # specify/enumerate the actions\n",
    "        self.terminate = 0\n",
    "        self.up = 1\n",
    "        self.down = 2\n",
    "        self.left = 3\n",
    "        self.right = 4\n",
    "        \n",
    "        # set the discount factor\n",
    "        self.gamma = discount_factor\n",
    "\n",
    "        # specify rewards\n",
    "        self.rewards =  {(3,1) : -1, (3,2) : 1}\n",
    "\n",
    "        # specify action cost \n",
    "        self.action_cost = 0.0\n",
    "\n",
    "        # empty list for storing the discounted reward at each step opf the episode\n",
    "        self.episode_discounted_rewards = []\n",
    "\n",
    "        # initialize Q-function and value function\n",
    "        self.Q = {}\n",
    "        self.V = {}\n",
    "        self.Vtemp = {}\n",
    "        for state in (self.states + [self.exit]):\n",
    "            self.Q[(state, self.up)] = 0.0\n",
    "            self.Q[(state, self.down)] = 0.0\n",
    "            self.Q[(state, self.left)] = 0.0\n",
    "            self.Q[(state, self.right)] = 0.0\n",
    "            self.V[state] = 0.0\n",
    "            self.Vtemp[state] = 0.0\n",
    "\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states   \n",
    "\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return self.initial_state\n",
    "\n",
    "\n",
    "    def get_terminal_states(self):\n",
    "        return self.terminal_states\n",
    "\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        if state not in self.terminal_states:\n",
    "            actions = [self.up, self.down, self.left, self.right]\n",
    "        else:\n",
    "            # for terminal states, the only valid action is to 'terminate' the episode\n",
    "            actions = [self.terminate]\n",
    "        return actions        \n",
    "\n",
    "    \n",
    "    # for given state-action pair, returns possible successor states along with their corresponding transition probabilities\n",
    "    def get_transitions(self, state, action):\n",
    "        \n",
    "        # if we're in an terminal state, then the only allowed action is to transition into the 'exit' state which terminates the episode\n",
    "        if state in self.terminal_states:\n",
    "            if action == self.terminate:\n",
    "                return [(self.exit, 1.0)]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        # probability of not slipping\n",
    "        straight = 1 - 2*self.noise\n",
    "\n",
    "        transitions = []\n",
    "        (x,y) = state\n",
    "\n",
    "        if action == self.up:\n",
    "            transitions.append(self.valid_add(state, (x,y+1), straight))\n",
    "            transitions.append(self.valid_add(state, (x-1,y), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x+1,y), self.noise))\n",
    "\n",
    "        elif action == self.down:\n",
    "            transitions.append(self.valid_add(state, (x,y-1), straight))\n",
    "            transitions.append(self.valid_add(state, (x-1,y), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x+1,y), self.noise))\n",
    "\n",
    "        elif action == self.left:\n",
    "            transitions.append(self.valid_add(state, (x-1,y), straight))\n",
    "            transitions.append(self.valid_add(state, (x,y-1), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x,y+1), self.noise))\n",
    "\n",
    "        elif action == self.right:\n",
    "            transitions.append(self.valid_add(state, (x+1,y), straight))\n",
    "            transitions.append(self.valid_add(state, (x,y-1), self.noise))\n",
    "            transitions.append(self.valid_add(state, (x,y+1), self.noise))\n",
    "\n",
    "        return transitions\n",
    "    \n",
    "\n",
    "    def get_rewards(self, state, action, next_state):\n",
    "        if (state in self.terminal_states) and (next_state == self.exit):\n",
    "            reward = self.rewards[state]\n",
    "        else:\n",
    "            reward = self.action_cost\n",
    "\n",
    "        # store discounted reward for the step\n",
    "        step = len(self.episode_discounted_rewards)\n",
    "        self.episode_discounted_rewards.append(reward * (self.gamma**step))\n",
    "\n",
    "        return reward    \n",
    "\n",
    "\n",
    "    def is_exit(self, state):\n",
    "        return (state == self.exit)    \n",
    "\n",
    "\n",
    "    def get_discount_factor(self):\n",
    "        return self.gamma  \n",
    "\n",
    "    \n",
    "    def execute(self, state, action):\n",
    "        # get all transitions\n",
    "        transitions = self.get_transitions(state, action)\n",
    "        states = [tr[0] for tr in transitions]\n",
    "        probs = [tr[1] for tr in transitions]\n",
    "        # sample from the transitions to get the next state state\n",
    "        next_state = np.random.choice(states, size=1, p=probs)\n",
    "        reward = self.get_rewards(state, action, next_state)\n",
    "        return (next_state, reward)\n",
    "    \n",
    "\n",
    "    def valid_add(self, state, next_state, probability):\n",
    "\n",
    "        # check if next state is a wall\n",
    "        if next_state in self.walls:\n",
    "            return (state, probability)\n",
    "\n",
    "        # check if next state is off grid\n",
    "        (x,y) = next_state\n",
    "        if (x>=0 and x<=(self.width-1) and y>=0 and y<=(self.height-1)):\n",
    "            return (next_state, probability)\n",
    "        else:\n",
    "            return (state, probability)\n",
    "\n",
    "\n",
    "    def update_Q(self, state, action, Qnew):\n",
    "        self.Q[(state, action)] = Qnew\n",
    "    \n",
    "    \n",
    "    def update_V(self):\n",
    "        # copy from Vtemp\n",
    "        for state in self.states:\n",
    "            self.V[state] = self.Vtemp[state]\n",
    "\n",
    "\n",
    "    def extract_policy(self):\n",
    "        # extract a policy using the Q function\n",
    "        policy = {}\n",
    "        Qs = {}\n",
    "        for state in self.states:\n",
    "            Qs['up'] = self.Q[(state, self.up)] \n",
    "            Qs['down'] = self.Q[(state, self.down)]\n",
    "            Qs['left'] = self.Q[(state, self.left)] \n",
    "            Qs['right'] = self.Q[(state, self.right)]         \n",
    "            policy[state] = max(Qs, key=lambda k:Qs[k])\n",
    "            \n",
    "        return policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the value iteration loop\n",
    "\n",
    "theta = 0.001 # threshold\n",
    "\n",
    "def value_iteration(grid_world_mdp, num_iters):\n",
    "    states = grid_world_mdp.get_states()    \n",
    "    gamma = grid_world_mdp.get_discount_factor()\n",
    "    for i in range(num_iters):\n",
    "        print(f\"Iteration# {i}\")\n",
    "        d = 0.0 \n",
    "        for state in states:\n",
    "            #print(f\"state: {state}\")\n",
    "            actions = grid_world_mdp.get_actions(state)\n",
    "            Qsa = [] \n",
    "            for action in actions:\n",
    "                #print(f\"action: {action}\")\n",
    "                # get the transitions\n",
    "                transitions = grid_world_mdp.get_transitions(state, action)\n",
    "                #print(f\"transitions: {transitions}\")\n",
    "                # compute Q value\n",
    "                Qnew = 0.0\n",
    "                for tr in transitions:\n",
    "                    (next_state, p) = tr\n",
    "                    Qnew += p * (grid_world_mdp.get_rewards(state, action, next_state) + gamma * grid_world_mdp.V[next_state])\n",
    "                grid_world_mdp.update_Q(state, action, Qnew)\n",
    "                Qsa.append(Qnew)\n",
    "            Vnew = max(Qsa)    \n",
    "            grid_world_mdp.Vtemp[state] = Vnew  \n",
    "            # update delta        \n",
    "            d = max(d, abs(Vnew - grid_world_mdp.V[state]))\n",
    "        \n",
    "        # update the value function\n",
    "        grid_world_mdp.update_V()\n",
    "\n",
    "        # diagnostic updated Q values in cell (0,0)\n",
    "        #print(f\"Cell(0,0): V = {grid_world_mdp.V[(0,0)]}, Qup = {grid_world_mdp.Q[((0,0),1)]}, Qdown = {grid_world_mdp.Q[((0,0),2)]}, Qleft = {grid_world_mdp.Q[((0,0),3)]}, Qright = {grid_world_mdp.Q[((0,0),4)]}\")\n",
    "        #print(f\"Cell(2,2): V = {grid_world_mdp.V[(2,2)]} ,Qup = {grid_world_mdp.Q[((2,2),1)]}, Qdown = {grid_world_mdp.Q[((2,2),2)]}, Qleft = {grid_world_mdp.Q[((2,2),3)]}, Qright = {grid_world_mdp.Q[((2,2),4)]}\")\n",
    "\n",
    "        print(\"-----------------------\")\n",
    "        for y in range(grid_world_mdp.height-1, -1, -1):\n",
    "            for x in range(grid_world_mdp.width):\n",
    "                if (x,y) in grid_world_mdp.V:\n",
    "                    print(f\"{grid_world_mdp.V[(x,y)]: 0.2f}\", end=' ')\n",
    "                else:\n",
    "                    print(f\"{0.0: 0.2f}\", end=' ')\n",
    "            print(\"\")       \n",
    "        print(\"-----------------------\")\n",
    "\n",
    "\n",
    "        # stop value iteration if delta falls below threshold    \n",
    "        if d <= theta:\n",
    "            break    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration# 0\n",
      "-----------------------\n",
      " 0.00  0.00  0.00  1.00 \n",
      " 0.00  0.00  0.00 -1.00 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Iteration# 1\n",
      "-----------------------\n",
      " 0.00  0.00  0.72  1.00 \n",
      " 0.00  0.00  0.00 -1.00 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Iteration# 2\n",
      "-----------------------\n",
      " 0.00  0.52  0.78  1.00 \n",
      " 0.00  0.00  0.43 -1.00 \n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Iteration# 3\n",
      "-----------------------\n",
      " 0.37  0.66  0.83  1.00 \n",
      " 0.00  0.00  0.51 -1.00 \n",
      " 0.00  0.00  0.31  0.00 \n",
      "-----------------------\n",
      "Iteration# 4\n",
      "-----------------------\n",
      " 0.51  0.72  0.84  1.00 \n",
      " 0.27  0.00  0.55 -1.00 \n",
      " 0.00  0.22  0.37  0.13 \n",
      "-----------------------\n",
      "Iteration# 5\n",
      "-----------------------\n",
      " 0.59  0.73  0.85  1.00 \n",
      " 0.41  0.00  0.57 -1.00 \n",
      " 0.21  0.31  0.43  0.19 \n",
      "-----------------------\n",
      "Iteration# 6\n",
      "-----------------------\n",
      " 0.62  0.74  0.85  1.00 \n",
      " 0.50  0.00  0.57 -1.00 \n",
      " 0.34  0.36  0.45  0.24 \n",
      "-----------------------\n",
      "Iteration# 7\n",
      "-----------------------\n",
      " 0.63  0.74  0.85  1.00 \n",
      " 0.53  0.00  0.57 -1.00 \n",
      " 0.42  0.39  0.46  0.26 \n",
      "-----------------------\n",
      "Iteration# 8\n",
      "-----------------------\n",
      " 0.64  0.74  0.85  1.00 \n",
      " 0.55  0.00  0.57 -1.00 \n",
      " 0.46  0.40  0.47  0.27 \n",
      "-----------------------\n",
      "Iteration# 9\n",
      "-----------------------\n",
      " 0.64  0.74  0.85  1.00 \n",
      " 0.56  0.00  0.57 -1.00 \n",
      " 0.48  0.41  0.47  0.27 \n",
      "-----------------------\n",
      "Iteration# 10\n",
      "-----------------------\n",
      " 0.64  0.74  0.85  1.00 \n",
      " 0.56  0.00  0.57 -1.00 \n",
      " 0.48  0.42  0.47  0.27 \n",
      "-----------------------\n",
      "Iteration# 11\n",
      "-----------------------\n",
      " 0.64  0.74  0.85  1.00 \n",
      " 0.57  0.00  0.57 -1.00 \n",
      " 0.49  0.42  0.47  0.28 \n",
      "-----------------------\n",
      "Iteration# 12\n",
      "-----------------------\n",
      " 0.64  0.74  0.85  1.00 \n",
      " 0.57  0.00  0.57 -1.00 \n",
      " 0.49  0.43  0.47  0.28 \n",
      "-----------------------\n",
      "Iteration# 13\n",
      "-----------------------\n",
      " 0.64  0.74  0.85  1.00 \n",
      " 0.57  0.00  0.57 -1.00 \n",
      " 0.49  0.43  0.47  0.28 \n",
      "-----------------------\n",
      "Iteration# 14\n",
      "-----------------------\n",
      " 0.64  0.74  0.85  1.00 \n",
      " 0.57  0.00  0.57 -1.00 \n",
      " 0.49  0.43  0.48  0.28 \n",
      "-----------------------\n",
      "Iteration# 15\n",
      "-----------------------\n",
      " 0.64  0.74  0.85  1.00 \n",
      " 0.57  0.00  0.57 -1.00 \n",
      " 0.49  0.43  0.48  0.28 \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# instantiate grid world mdp object\n",
    "gw = GridWorld(discount_factor=0.9)\n",
    "\n",
    "value_iteration(gw, num_iters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy extraction\n",
    "pi = gw.extract_policy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "right right right up \n",
      "up None up up \n",
      "up left up left \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------\")\n",
    "for y in range(gw.height-1, -1, -1):\n",
    "    for x in range(gw.width):\n",
    "        if (x,y) in pi:\n",
    "            print(f\"{pi[(x,y)]}\", end=' ')\n",
    "        else:\n",
    "            print(f\"{'None'}\", end=' ')\n",
    "    print(\"\")       \n",
    "print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning: For model free MDPs, we cxannot use value iteration. Instead, we can bootstrap from an estimate of the value function to update Q values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2139099261.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    def bandit(self, state):\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# create a Q-learner class\n",
    "class QLearner:\n",
    "    def __init__(self, mdp, alpha=0.1) :\n",
    "        self.mdp = mdp\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = 0.1\n",
    "        np.random.seed(2)\n",
    "\n",
    "\n",
    "    # epsilon-greedy multi-arm bandit\n",
    "    def bandit(self, state):    \n",
    "        # get all available actions\n",
    "        actions = self.mdp.get_actions(state)\n",
    "        randnum = np.random.random()\n",
    "        # select next actions via exploration-exploitation\n",
    "        if randnum < self.epsilon:\n",
    "            action = np.random.sample(actions, size=1)\n",
    "        else:\n",
    "            # get Q values\n",
    "            Qsa = {action:self.mdp.Q[(state, action)] for action in actions}\n",
    "            # argmax to find best action\n",
    "            action = max(Qsa, key=Qsa.get)        \n",
    "        return action\n",
    "\n",
    "\n",
    "    # Q-learning update\n",
    "    def get_delta(self, reward, Qold, state, next_state, next_action):\n",
    "        # get estimated value for next state\n",
    "        aprime = self.mdp.get_actions(next_state)\n",
    "        Vsprime = max([self.mdp.Q[(next_state, action)] for action in aprime])\n",
    "        delta = reward + self.mdp.gamma * Vsprime - Qold\n",
    "        return self.alpha * delta  \n",
    "\n",
    "\n",
    "    # Q-learner training loop\n",
    "    def train(self, episodes=10):\n",
    "\n",
    "        for i in range(episodes):\n",
    "            # get initial state for the episode\n",
    "            state = self.mdp.get_initial_state()\n",
    "            # select an action using bandit\n",
    "            action = self.bandit(state)\n",
    "            # repeat until terminal state is reached\n",
    "            while not self.mdp.is_exit(state):\n",
    "                (next_state, reward) = self.mdp.execute(state, action)\n",
    "                next_action = self.bandit(next_state)\n",
    "                # update q value\n",
    "                Qold = self.mdp.Q[(state, action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_planning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
